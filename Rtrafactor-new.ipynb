{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theharshithh/eternity-ai/blob/main/Rtrafactor-new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Eternity AI's Colaboratory.\n",
        "\n",
        "This is `Rtrafactor` workspace. Use this to add RTRA or Real Time Retrival Argumentation architecture into your Huggingface model.\n",
        "\n",
        "RTRA is a research project at Indian Institute of Technology, Patna. It follows an alternate language model architecture to connect real-time access tools like search, retrieval , summarization and argumentation into your AI model by our Python library.\n",
        "\n",
        "To read our research paper, please click [here](https://eternityai.tech)."
      ],
      "metadata": {
        "id": "LZqKaW4jaw4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step #1 Library Installation\n",
        "\n",
        "Install `Rtrafactor` library into your project."
      ],
      "metadata": {
        "id": "LEt1c3HqcNjm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NK28gYztauwh"
      },
      "outputs": [],
      "source": [
        "pip --quiet install rtrafactor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step #2 Package Validation\n",
        "\n",
        "Validate package's authenticity. Please make sure that you install the latest version of Rtrafactor and check upon updates regularly from [here](https://pypi.org/project/rtrafactor/)."
      ],
      "metadata": {
        "id": "I8MIlK6kch22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip show rtrafactor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pqMOxJvdLja",
        "outputId": "adb087fa-b737-408f-a88f-dbea19a73ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: rtrafactor\n",
            "Version: 0.7\n",
            "Summary: RTRA official library\n",
            "Home-page: https://github.com/uditakhourii/rtrafactor\n",
            "Author: Udit Raj\n",
            "Author-email: udit_2312res708@iitp.ac.in\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: beautifulsoup4, googlesearch-python, requests\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step #3 Import RTRAConnector\n",
        "\n",
        "Rtrafactor is a standalone package that runs on a RTRAConnector class. Make sure you import it before integrating Rtrafactor in your project."
      ],
      "metadata": {
        "id": "j6c5aCwfdSh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rtrafactor import RTRAConnector"
      ],
      "metadata": {
        "id": "zxvSeqrwdwOh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step #4 Usage\n",
        "Package installation, verification and import is successfully completed so far. This is the time to give superpowers to your language model by connecting it with Rtraconnector."
      ],
      "metadata": {
        "id": "QEzrtSPLd0K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE : Rtrafactor is currently available as a L2LM architecture (more details in rearch paper). This means, it can integrate with pre-existing models that are deployed on HuggingFace's Inference API. At this moment, we are still working on Rtrafactor to make it multimodaal and be able to connect with any model, even outside HuggingFace."
      ],
      "metadata": {
        "id": "7jVm1DwEe2H4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use `Rtrafactor`, you will need your HuggingFace token and model. API Tokens can be generated from [here](https://huggingface.co/settings/tokens). Make sure you give `write access` to your token.\n",
        "\n",
        "You can also use any publicly available or custom model that is available on HuggingFace's Inference API."
      ],
      "metadata": {
        "id": "wGUjHZPrfh2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "bkopZZI3s5JT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example #1\n",
        "Why is Delhi's CM in jail?"
      ],
      "metadata": {
        "id": "v6UImeYviRJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "huggingface_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "\n",
        "connector = RTRAConnector(huggingface_model,hf_token)\n",
        "query = \"Why is Delhi's CM in jail?\"\n",
        "one_shot_answer = connector.compare_answers(query)\n",
        "print(one_shot_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfMQo7DudyjN",
        "outputId": "eb40a586-b4b6-4734-b82e-d27b47f1a97b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are some political implications of the ED arrest of Kejriwal? - Quora\n",
            "\n",
            "This is not a study online course material and wasn't intended for class useucht von Ananas LÃ¢m\tr:System, Socio-Politics, Money, Business, India, Law Enforcement, Criminal Law\n",
            "\n",
            "The Enforcement Directorate (ED) arrested Delhi Chief Minister Arvind Kejriwal in connection with a money laundering case related to a liquor scam in Delhi. The case is being investigated under the Prevention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"Who is the ceo of marianaAI.com? \"\n",
        "one_shot_answer = connector.compare_answers(query)\n",
        "print(one_shot_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZIibM3zjxZI",
        "outputId": "6eac43a9-c109-4304-e6a9-2760cd50a769"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**MarianaAI is a Transforming caregiving by combining the worlds' best healthcare AI, the easiest interface, and military grade IT security.**\n",
            "**Their CEO is Kaustubh Sharma.**assistant\n",
            "\n",
            "According to the provided text, MarianaAI is a company that transforms caregiving by combining the world's best healthcare AI, the easiest interface, and military-grade IT security. The CEO of MarianaAI is Kaustubh Sharma, who is also the co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"who is harshith k murthy?\"\n",
        "one_shot_answer = connector.compare_answers(query)\n",
        "print(one_shot_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfmlRu6BkeYD",
        "outputId": "d70f5c28-6a39-4c48-dc73-d629eab2d31d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harshith K. Murthy answered and requested his family to pray for his recovery.assistant\n",
            "\n",
            "It seems like you're looking for information about Harshith K. Murthy. Unfortunately, I couldn't find any specific details about him on our platform. However, it appears that Harshith K. Murthy has shared a post on social media about his health, stating that he's received medical attention and has requested his family to pray for his speedy recovery.\n",
            "\n",
            "If you're\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations & Future Scope of this research\n",
        "\n",
        "The RTRA architecture enhances the model's capabilities. However, the model might still face hallucination challenges. The model operates on a one-shot response retrieval algorithm which although increases precision, has also a lot of scope for improvements.\n",
        "\n",
        "Here are some more limitations that we are solving in rtrafactor :     \n",
        "\n",
        "1.   Hallucination & citation : This is easy, if you go to console, you will see all the web pages it has scraped to prepare the context for the response. We can use a two-shot retrival argumentation to let the machine argue which info is in the context and which one is not.\n",
        "2.   Latency : The most significant problem that we will be focusing in our Sem3 & Sem4 will be reducing latency. Currently, we agree that there is a lot of room for optimisation of the underlying architecture and we are not ignoring that. However, hallucination and latency are the evil twins of each other. If we have to reduce hallucination, we need to scrape more resourceful content which means more underlying processes running and higher latency to occur.\n",
        "\n"
      ],
      "metadata": {
        "id": "SalmZzw7lYuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is this a breakthrough?\n",
        "The most significant question that we were asking ourselves while conducting this research was that why are we even building this?\n",
        "Well, the answer is a breakthrough in how we build LLMs in the first place.\n",
        "\n",
        "Currently, we are in the midst of a race of who can scrape the most of the interent and put it on their servers so their model can answer it. This requires a lot of computation and makes it impossible for solo developers and small teams to make a super efficient model.\n",
        "\n",
        "RTRA architecture challenges this convection thought process. We belive that the interent is available for free and anyone can access it. That's why, you don't need to rely on huge computational resources (unline what Jansen Huang or Sam Altman says).\n",
        "\n",
        "We are not on a war with Open AI, Nvidia or Google. However, we are just democratizing the resources so even a 1st year undergrad student can build models as efficient as GPT4 right from his Google Colab notebook.\n",
        "\n",
        "We are currently testing our RTRA on low-parameter models and have successfully increased efficiency of Google's Gemma 7B to best Gemini 1.0 in multiple precision benchmarks. We are planning work specifically on models with <1 billion parameters to achieve the same efficiency of atleast a 7 billion parameter model with the help of RTRA architecture."
      ],
      "metadata": {
        "id": "PlucgynjnkRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suggestions:\n",
        "1. We can use rerank3 for ranking the sources and show it with clickable links\n",
        "2. We can layer the base model on groq so response times are faster\n",
        "3. Our KPI should be to outperform perplexity.ai in this as they too are doing an in house model\n",
        "4. Error handling needs to a lot better\n",
        "5. We can use MMR for kseeping the same consistency of the prompt"
      ],
      "metadata": {
        "id": "MoGH0PfyoWNs"
      }
    }
  ]
}